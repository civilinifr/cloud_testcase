{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "145d095d-1e5e-4b29-a179-111f7fd06f97",
   "metadata": {},
   "source": [
    "# PDS Cloud Pilot Study Preliminary Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d05e2b-141b-4718-b5cf-d5659ea8c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from obspy.core import read\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032fca2c-bb95-44c7-90b9-616d9f0363da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we will select a single day file within our github directory for convenience\n",
    "# There are three files in the directory, corresponding to the same day for a single instrument. \n",
    "# The three files correspond to the three recorded directions of motion (2 horizontal, one vertical)\n",
    "\n",
    "# testday_files = sorted(glob.glob(f'{datadir}*.mseed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d4bd2-ad74-4375-aee6-35146cc55b2d",
   "metadata": {},
   "source": [
    "## Alternate way of pulling the data using the PDS API\n",
    "\n",
    "Pull the collection of data files, relevant for a given day, here as in the example \"1970-084\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5d382-bca2-4775-814c-d0689c82b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pds.peppi as pep\n",
    "\n",
    "client = pep.PDSRegistryClient()\n",
    "\n",
    "day = datetime.strptime(\"1970-084\", \"%Y-%j\")\n",
    "products = pep.Products(client).of_collection(\"urn:nasa:pds:apollo_pse:data_seed::1.0\").after(day).before(day)\n",
    "\n",
    "seismic_data_products = [p for p in products if p.properties[\"apollo:Seismic_Parameters.apollo:pse_data_type\"][0] == \"waveform\"]\n",
    "for p in seismic_data_products:\n",
    "    print(p.id, p.properties[\"apollo:Seismic_Parameters.apollo:channel\"][0], p.properties[\"ops:Data_File_Info.ops:file_ref\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9050790-a8c3-400e-bff0-72c937685665",
   "metadata": {},
   "outputs": [],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a58e00-f40b-48e9-a5f6-95c012cc14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in seismic_data_products:\n",
    "    print(p.properties[\"ops:Data_File_Info.ops:file_ref\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af276056-b5b0-40e3-a3c0-8d8735730a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reading the data from the URL without downloading it\n",
    "import urllib\n",
    "\n",
    "test_url = seismic_data_products[0].properties[\"ops:Data_File_Info.ops:file_ref\"][0]\n",
    "st = read(test_url)\n",
    "st[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa1cf5-d2d6-4d6f-9d76-e1d69a1ec876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "import urllib\n",
    "\n",
    "locdir = './download_test/'\n",
    "if not os.path.exists(locdir):\n",
    "    os.mkdir(locdir)\n",
    "for p in seismic_data_products:\n",
    "    if not os.path.exists(f'{locdir}{os.path.basename(p.properties[\"ops:Data_File_Info.ops:file_ref\"][0])}'):\n",
    "        urllib.request.urlretrieve(p.properties[\"ops:Data_File_Info.ops:file_ref\"][0],\n",
    "                                   f'{locdir}{os.path.basename(p.properties[\"ops:Data_File_Info.ops:file_ref\"][0])}')\n",
    "        print(f'Downloaded {os.path.basename(p.properties[\"ops:Data_File_Info.ops:file_ref\"][0])}!')\n",
    "    else:\n",
    "        print(f'File {os.path.basename(p.properties[\"ops:Data_File_Info.ops:file_ref\"][0])} already downloaded!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a413a8e6-9938-4a06-8cd6-4b2f5a8e1c5c",
   "metadata": {},
   "source": [
    "### Product details\n",
    "\n",
    "Each product comes with a lot of metadata (originally managed in the PDS4 XML labels).\n",
    "\n",
    "If needed for this use case, we could use these fields, especially, the properties specific to the apollo mission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3ff06-2062-4e68-8a2a-22921efa3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in p.properties.items():\n",
    "    if k.startswith(\"apollo:\"):\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee53ab-4765-4fed-9c7f-d90be5dd9525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f4b54-544c-4bc8-932e-b9f1793eb418",
   "metadata": {},
   "source": [
    "# Test 1: Read in the raw data and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f80f8-e85d-4712-8dc1-351acbb9f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tracelen(infile):\n",
    "    \"\"\"\n",
    "    Finds the length of the trace so we can pre-load the data matrix for analysis\n",
    "    Also returns the start time of the file in datetime format\n",
    "    \n",
    "    :param infile: [str] Path to file to read\n",
    "    \"\"\"\n",
    "    # Read the data as a stream and extract the trace\n",
    "    st = read(infile)\n",
    "    tr = st[0]\n",
    "    \n",
    "    return len(tr.data), tr.stats.starttime.datetime\n",
    "    \n",
    "def read_mseed(infile):\n",
    "    \"\"\"\n",
    "    Reads in the data using the obspy utility and returns the time and velocity measurements\n",
    "    \n",
    "    :param infile: [str] Path to file to read\n",
    "    \"\"\"\n",
    "    # Read the data as a stream and extract the trace\n",
    "    st = read(infile)\n",
    "    tr = st[0]\n",
    "    \n",
    "    return tr.times(), tr.data\n",
    "\n",
    "# First, return the length of the data for this trace and find the start time of the file (it should be the same for all components)\n",
    "len_trace, trace_start = find_tracelen(testday_files[0])\n",
    "\n",
    "# Create the array and place the input values\n",
    "# Make sure to save the channel names while we're cycling through\n",
    "valdata_array_raw = np.zeros((len_trace, len(testday_files)))\n",
    "channames = []\n",
    "\n",
    "for infile_ind in np.arange(len(testday_files)):\n",
    "    timedata, valdata = read_mseed(testday_files[infile_ind])\n",
    "    valdata_array_raw[:, infile_ind] = valdata\n",
    "    channames.append(os.path.basename(testday_files[infile_ind]).split('.')[3])\n",
    "day = f'{trace_start.year}-{str(trace_start.month).zfill(2)}-{str(trace_start.day).zfill(2)}'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3484cb3e-9c53-412a-9cc5-0ede40d29055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw data\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "for chan_ind in np.arange(np.shape(valdata_array_raw)[1]):\n",
    "    # Plot the raw data\n",
    "    ax0 = plt.subplot(1, np.shape(valdata_array_raw)[1], chan_ind+1)\n",
    "    ax0.plot(timedata, valdata_array_raw[:, chan_ind], color='gray')\n",
    "    ax0.set_xlim((timedata[0], timedata[-1]))\n",
    "    ax0.set_title(f'{channames[chan_ind]}')\n",
    "    ax0.set_xlabel('Time (sec)')\n",
    "    ax0.set_ylabel('Raw Velocity (digital counts)')\n",
    "fig.suptitle(f'3-channel Apollo 12 Seismic Data ({day})', fontweight='bold')\n",
    "fig.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e53f9-4a3b-4a6d-8c7d-ff2408f8c29a",
   "metadata": {},
   "source": [
    "# Test 2: Process the data from machine counts to physical units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5baba-fadf-45c6-8e93-c7c840f37a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some additional libraries\n",
    "from obspy.signal.invsim import cosine_taper\n",
    "from scipy.interpolate import interp1d\n",
    "from obspy import read_inventory\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6d07f-15ad-4b7a-8b26-f473c73abc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some subroutines required for the data processing\n",
    "def running_median(seq, win):\n",
    "    \"\"\"\n",
    "    Conducts a running median on the data\n",
    "\n",
    "    :param seq: [Vector] Input data\n",
    "    :param win: [Integer] Size of the window (in samples)\n",
    "    \"\"\"\n",
    "\n",
    "    samples = np.arange(len(seq))\n",
    "    medians = []\n",
    "\n",
    "    window_middle = int(np.ceil(win / 2))\n",
    "\n",
    "    for ind in np.arange(len(seq)):\n",
    "\n",
    "        if ind <= window_middle:\n",
    "            medians.append(np.median(abs(seq[0:win])))\n",
    "\n",
    "        if ind >= len(seq) - window_middle:\n",
    "            medians.append(np.median(abs(seq[len(seq) - win:len(seq)])))\n",
    "\n",
    "        if window_middle < ind < len(seq) - window_middle:\n",
    "            medians.append(np.median(abs(seq[ind - int(np.floor(win / 2)):ind + int(np.floor(win / 2))])))\n",
    "\n",
    "    return np.array(medians)\n",
    "\n",
    "\n",
    "def despike(input_t, input_d, fs):\n",
    "    \"\"\"\n",
    "    Despikes the data according to Budlow 2005\n",
    "\n",
    "    :param input_t: [Vector] Interpolated time\n",
    "    :param input_d: [Vector] Interpolated data\n",
    "    :param fs: [Float] Sampling frequency\n",
    "    :param instrument_type: [String] Type of instrument [lp = long period, sp = short period]\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute a running median on the data\n",
    "    # The window size should be 2 minutes (120 seconds) and odd\n",
    "    window_size = int(fs * 120)\n",
    "    if window_size % 2 == 0:\n",
    "        window_size = window_size + 1\n",
    "    med = running_median(input_d, window_size)\n",
    "\n",
    "    # Find values greater than 5 times the running median\n",
    "    med_multiplier = 5.\n",
    "    indices_to_remove = []\n",
    "    for ind in np.arange(len(input_d)):\n",
    "        if input_d[ind] > abs(med[ind] * med_multiplier) or input_d[ind] < -1 * abs(med[ind] * med_multiplier):\n",
    "            indices_to_remove.append(ind)\n",
    "\n",
    "    # Remove those values from the time and data\n",
    "    input_t_del = np.delete(input_t, indices_to_remove)\n",
    "    input_d_del = np.delete(input_d, indices_to_remove)\n",
    "\n",
    "    # If we remove the last value in the dat, we run into trouble because it can't finish interpolation\n",
    "    # If it's missing, append a zero value to the data at the end. We have a total of four cases.\n",
    "    # Missing beginning\n",
    "    if not input_t_del[0] == input_t[0] and input_t_del[-1] == input_t[-1]:\n",
    "        input_t_del_fin = np.insert(input_t_del, 0, input_t[0])\n",
    "        input_d_del_fin = np.insert(input_d_del, 0, 0)\n",
    "\n",
    "    # Missing end\n",
    "    if input_t_del[0] == input_t[0] and not input_t_del[-1] == input_t[-1]:\n",
    "        input_t_del_fin = np.append(input_t_del, input_t[-1])\n",
    "        input_d_del_fin = np.append(input_d_del, 0)\n",
    "\n",
    "    # Both missing\n",
    "    if not input_t_del[0] == input_t[0] and not input_t_del[-1] == input_t[-1]:\n",
    "        input_t_del_fixbeg = np.insert(input_t_del, 0, input_t[0])\n",
    "        input_d_del_fixbeg = np.insert(input_d_del, 0, 0)\n",
    "        input_t_del_fin = np.append(input_t_del_fixbeg, input_t[-1])\n",
    "        input_d_del_fin = np.append(input_d_del_fixbeg, 0)\n",
    "\n",
    "    # Nothing missing\n",
    "    if input_t_del[0] == input_t[0] and input_t_del[-1] == input_t[-1]:\n",
    "        input_t_del_fin = input_t_del\n",
    "        input_d_del_fin = input_d_del\n",
    "\n",
    "    # Interpolate over the missing values\n",
    "    # We can call on our original input_t variable\n",
    "    f2 = interp1d(input_t_del_fin, input_d_del_fin)\n",
    "    d_interp2 = f2(input_t)\n",
    "\n",
    "    return d_interp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a6df2-1fdc-41ec-9205-1f90a2f8854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(indata, dataless_seed):\n",
    "    \"\"\"\n",
    "    Processes the data: removes mean, tapers, and removes the instrument response (converting it to physical units)\n",
    "    \n",
    "    :param infile: [str] Path to file to read\n",
    "    :param dataless_seed: [str] Path to the dataless seed, which is required for removing the instrument response\n",
    "    \"\"\"\n",
    "    # Read the data as a stream and extract the trace\n",
    "    st = read(indata)\n",
    "    tr = st[0]\n",
    "    \n",
    "    # Remove the mean and run a cosine taper\n",
    "    tr.data = tr.data - np.mean(tr.data)\n",
    "    taper_function = cosine_taper(len(tr.data), p=0.02)\n",
    "    tr.data = tr.data * taper_function\n",
    "\n",
    "    # Set a bandpass filter and remove the instrument response\n",
    "    # Note: This narrow filter is required for instrument response removal. Otherwise the low frequencies dominate the spectrum. \n",
    "    pre_filt = [0.1, 0.3, 0.9, 1.1]\n",
    "    inv = read_inventory(dataless_seed)\n",
    "    tr.remove_response(inventory=inv, pre_filt=pre_filt, output=\"VEL\",\n",
    "                       water_level=None)\n",
    "\n",
    "    # Interpolate and despike the data\n",
    "    sr = 6.625\n",
    "    delta_target = 1 / sr\n",
    "    tm = tr.times()\n",
    "    times_seconds_interp = np.arange(tm[0], tm[-1] - delta_target, delta_target)\n",
    "    tm_utc = tr.times(type='utcdatetime')\n",
    "    tm_num_interp = [tm_utc[0] + dt.timedelta(seconds=x) for x in\n",
    "                        times_seconds_interp]\n",
    "\n",
    "    f = interp1d(tm, tr.data)\n",
    "    d_interp = f(times_seconds_interp)\n",
    "\n",
    "    d_new = despike(times_seconds_interp, d_interp, 1 / delta_target)\n",
    "\n",
    "    print(f'Processed file {os.path.basename(indata)}')\n",
    "\n",
    "    return times_seconds_interp, d_new\n",
    "\n",
    "# Pass the location of the dataless seed which contains the information for instrument response removal\n",
    "dataless_seed = f'{rundir}dataless.xa.0.seed'\n",
    "\n",
    "# Cycle through each file and process the data\n",
    "for infile_ind in np.arange(len(testday_files)):\n",
    "    proctime, procdata = process_data(testday_files[infile_ind], dataless_seed)\n",
    "\n",
    "    # Since we are interpolating, the size of our data is going to be different, so we have to wait until the first interpolation to create the matrix\n",
    "    procdata_len = len(procdata)\n",
    "\n",
    "    if infile_ind == 0:\n",
    "        valdata_array_proc = np.zeros((procdata_len, len(testday_files)))\n",
    "    valdata_array_proc[:, infile_ind] = procdata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f96e9-c29c-4799-a950-9b3f4614a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "for chan_ind in np.arange(np.shape(valdata_array_raw)[1]):\n",
    "    # Plot the raw data\n",
    "    ax0 = plt.subplot(2, np.shape(valdata_array_raw)[1], chan_ind+1)\n",
    "    ax0.plot(timedata, valdata_array_raw[:, chan_ind], c='gray')\n",
    "    ax0.set_xlim((timedata[0], timedata[-1]))\n",
    "    ax0.set_title(f'{channames[chan_ind]}')\n",
    "    ax0.set_xlabel('Time (sec)')\n",
    "    ax0.set_ylabel('Raw Velocity (digital counts)')\n",
    "\n",
    "    # Plot the processed data\n",
    "    ax1 = plt.subplot(2, np.shape(valdata_array_raw)[1], chan_ind+4)\n",
    "    ax1.plot(proctime, valdata_array_proc[:, chan_ind])\n",
    "    ax1.set_xlim((proctime[0], proctime[-1]))\n",
    "    ax1.set_xlabel('Time (sec)')\n",
    "    ax1.set_ylabel('Processed Velocity (m/s)')\n",
    "fig.suptitle(f'3-channel Apollo 12 Seismic Data ({day})', fontweight='bold')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366888d0-0872-409e-921e-9f0491261c41",
   "metadata": {},
   "source": [
    "# Test 3: Compute Spectrograms for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5540bec-0b07-4f01-80a7-abb2a71c9f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional packages\n",
    "from scipy import signal\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081bd1c-136c-4e86-9585-3ef9e6af322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to compute the seismic arrival time of an impact moonquake that occurs in this hour\n",
    "# We use the term evid (event ID) to describe the arrival\n",
    "# The format we will use for datetime is %Y-%m-%dT%H:%M:%S\n",
    "arrival_absolute_str = '1970-03-25T3:32:00'\n",
    "arrival_absolute_dt = dt.datetime.strptime(arrival_absolute_str, '%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "# Convert it to the time in seconds after the start of the hour\n",
    "arrival_rel = (arrival_absolute_dt - trace_start).seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15fe7f3-0566-460b-b3c1-92099d3fd1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spec(trtime, trvals):\n",
    "    \"\"\"\n",
    "    Computes the spectrograms of the processed data\n",
    "\n",
    "    :param trtime: [vector] Processed time values\n",
    "    :param trvals: [vector] Processed velocity values\n",
    "    \"\"\"\n",
    "    # Get the sample-rate of the processed data. If we don't have an explicit value saved, we can just use differences in the time vector\n",
    "    delta_samples = trtime[1]-trtime[0]\n",
    "    sampling_rate = 1/delta_samples\n",
    "    \n",
    "    # Compute spectrogram\n",
    "    spec_f, spec_t, spec_sxx = signal.spectrogram(trvals, sampling_rate)\n",
    "    \n",
    "    return spec_f, spec_t, spec_sxx\n",
    "\n",
    "# Cycle through each file \n",
    "for chanind in np.arange(np.shape(valdata_array_proc)[1]):\n",
    "    spec_f, spec_t, spec_sxx = compute_spec(proctime, valdata_array_proc[:, chanind])\n",
    "\n",
    "    # Create the output vector. Since it's an array instead of a vector, our final output needs to be a 3D array\n",
    "    if chanind == 0:\n",
    "        spec_array = np.zeros((np.shape(spec_sxx)[0], np.shape(spec_sxx)[1], len(testday_files)))\n",
    "    spec_array[:, :, chanind] = spec_sxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f757355c-b8a3-4eee-8f6c-8e3d1c2ea660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "for chan_ind in np.arange(np.shape(valdata_array_raw)[1]):\n",
    "    # Plot the raw data\n",
    "    ax0 = plt.subplot(3, np.shape(valdata_array_raw)[1], chan_ind+1)\n",
    "    ax0.plot(timedata, valdata_array_raw[:, chan_ind], c='gray')\n",
    "    ax0.set_xlim((timedata[0], timedata[-1]))\n",
    "    ax0.set_title(f'{channames[chan_ind]}')\n",
    "    ax0.set_xlabel('Time (sec)')\n",
    "    ax0.set_ylabel('Raw Velocity (digital counts)')\n",
    "\n",
    "    # Plot the processed data\n",
    "    ax1 = plt.subplot(3, np.shape(valdata_array_raw)[1], chan_ind+4)\n",
    "    ax1.plot(proctime, valdata_array_proc[:, chan_ind])\n",
    "    ax1.set_xlim((proctime[0], proctime[-1]))\n",
    "    ax1.set_xlabel('Time (sec)')\n",
    "    ax1.set_ylabel('Processed Velocity (m/s)')\n",
    "\n",
    "    # Plot the spectrogram\n",
    "    ax2 = plt.subplot(3, np.shape(valdata_array_raw)[1], chan_ind+7)\n",
    "    ax2.pcolormesh(spec_t, spec_f, spec_sxx, cmap=cm.jet, vmax=5e-18)\n",
    "    ax2.set_xlabel('Time (sec)')\n",
    "    ax2.set_ylabel('Frequency (Hz)')\n",
    "\n",
    "    # Plot the arrivals\n",
    "    ax0.axvline(arrival_rel, c='red')\n",
    "    ax1.axvline(arrival_rel, c='red')\n",
    "    ax2.axvline(arrival_rel, c='red')\n",
    "    \n",
    "fig.suptitle(f'3-channel Apollo 12 Seismic Data', fontweight='bold')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a37df56-9176-480a-b0e8-940a3bb38455",
   "metadata": {},
   "source": [
    "# Test 4: Cut and export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571c892b-3f2e-4d53-8c65-e4dfb1b97bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import more packages\n",
    "from obspy import UTCDateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bb197-1b5d-4124-b708-48563dfac5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a trim length before and after the arrival (in seconds)\n",
    "pre_arrival_time = 300\n",
    "post_arrival_time = 5200\n",
    "\n",
    "# Use the timedelta function to get the absolute datetime of when to cut the trace\n",
    "pre_arrival_dt = arrival_absolute_dt - dt.timedelta(seconds=pre_arrival_time)\n",
    "post_arrival_dt = arrival_absolute_dt + dt.timedelta(seconds=post_arrival_time)\n",
    "\n",
    "# Obspy uses utctime, so use the UTCDateTime function to convert it to that format\n",
    "pre_arrival_utc = UTCDateTime(pre_arrival_dt)\n",
    "post_arrival_utc = UTCDateTime(post_arrival_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c0583-87b9-416c-9e56-775478925199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an output directory\n",
    "outdir = f'{rundir}test_output/'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "# Cut the files\n",
    "for chandir in np.arange(len(testday_files)):\n",
    "    bname = os.path.basename(testday_files[chandir])\n",
    "    st = read(testday_files[chandir])\n",
    "    st.trim(pre_arrival_utc, post_arrival_utc)\n",
    "    st.write(f'{outdir}{bname[0:-6]}_cut.mseed', format='MSEED')\n",
    "    print(f'Cut file {bname[0:-6]}_cut.mseed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808168b-dcee-4019-b021-99ba096b3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cut files and plot their new raw values\n",
    "cutfiles = sorted(glob.glob(f'{outdir}*.mseed'))\n",
    "\n",
    "# Find their new cut length\n",
    "cutlen_trace, cuttrace_start = find_tracelen(cutfiles[0])\n",
    "\n",
    "# Add the values to a new raw array\n",
    "valdata_array_rawcut = np.zeros((cutlen_trace, len(cutfiles)))\n",
    "for infile_ind in np.arange(len(cutfiles)):\n",
    "    timedata_cut, valdata_cut = read_mseed(cutfiles[infile_ind])\n",
    "    valdata_array_rawcut[:, infile_ind] = valdata_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f06d2b-1dbf-494a-a05d-6020c7e96034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same processing to the new files\n",
    "for infile_ind in np.arange(len(cutfiles)):\n",
    "    cuttime, cutdata = process_data(cutfiles[infile_ind], dataless_seed)\n",
    "\n",
    "    # Since we are interpolating, the size of our data is going to be different, so we have to wait until the first interpolation to create the matrix\n",
    "    cutdata_len = len(cutdata)\n",
    "\n",
    "    if infile_ind == 0:\n",
    "        valdata_array_cut = np.zeros((cutdata_len, len(cutfiles)))\n",
    "    valdata_array_cut[:, infile_ind] = cutdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905b8b3-676a-4e7d-93bc-ea1c1846741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the new spectrogram\n",
    "for chanind in np.arange(np.shape(valdata_array_cut)[1]):\n",
    "    speccut_f, speccut_t, speccut_sxx = compute_spec(cuttime, valdata_array_cut[:, chanind])\n",
    "\n",
    "    # Create the output vector. Since it's an array instead of a vector, our final output needs to be a 3D array\n",
    "    if chanind == 0:\n",
    "        spec_array_cut = np.zeros((np.shape(speccut_sxx)[0], np.shape(speccut_sxx)[1], len(cutfiles)))\n",
    "    spec_array_cut[:, :, chanind] = speccut_sxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bdb387-de0c-4139-86b7-eba595cf113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the new relative seismic arrival time\n",
    "arrival_relcut = (arrival_absolute_dt - cuttrace_start).seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36323995-fdd2-4851-baac-9a0e2351dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "for chan_ind in np.arange(np.shape(valdata_array_rawcut)[1]):\n",
    "    # Plot the raw data\n",
    "    ax0 = plt.subplot(3, np.shape(valdata_array_rawcut)[1], chan_ind+1)\n",
    "    ax0.plot(timedata_cut, valdata_array_rawcut[:, chan_ind], c='gray')\n",
    "    ax0.set_xlim((timedata_cut[0], timedata_cut[-1]))\n",
    "    ax0.set_title(f'{channames[chan_ind]}')\n",
    "    ax0.set_xlabel('Time (sec)')\n",
    "    ax0.set_ylabel('Raw Velocity (digital counts)')\n",
    "\n",
    "    # Plot the processed data\n",
    "    ax1 = plt.subplot(3, np.shape(valdata_array_rawcut)[1], chan_ind+4)\n",
    "    ax1.plot(cuttime, valdata_array_cut[:, chan_ind])\n",
    "    ax1.set_xlim((cuttime[0], cuttime[-1]))\n",
    "    ax1.set_xlabel('Time (sec)')\n",
    "    ax1.set_ylabel('Processed Velocity (m/s)')\n",
    "\n",
    "    # Plot the spectrogram\n",
    "    ax2 = plt.subplot(3, np.shape(valdata_array_rawcut)[1], chan_ind+7)\n",
    "    ax2.pcolormesh(speccut_t, speccut_f, speccut_sxx, cmap=cm.jet, vmax=5e-18)\n",
    "    ax2.set_xlabel('Time (sec)')\n",
    "    ax2.set_ylabel('Frequency (Hz)')\n",
    "\n",
    "    # Plot the arrivals\n",
    "    ax0.axvline(arrival_relcut, c='red')\n",
    "    ax1.axvline(arrival_relcut, c='red')\n",
    "    ax2.axvline(arrival_relcut, c='red')\n",
    "    \n",
    "fig.suptitle(f'3-channel Apollo 12 Seismic Data', fontweight='bold')\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
