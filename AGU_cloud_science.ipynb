{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "145d095d-1e5e-4b29-a179-111f7fd06f97",
   "metadata": {},
   "source": [
    "# AGU PDS Cloud Science Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d05e2b-141b-4718-b5cf-d5659ea8c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from obspy.core import read\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import datetime as dt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c505762d-8e2f-421e-8347-02cbbb986850",
   "metadata": {},
   "source": [
    "In this demo, we will show how to download seismic data from the apollo missions and run a seismic detector code on it. \n",
    "\n",
    "In this example, we will use xa.s12.00.mhz.1970-03-25\n",
    "\n",
    "We will show operations on how to conduct research using a single piece of selected data without having to explicitly download it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23a105-bec1-432d-a71e-170969b329e7",
   "metadata": {},
   "source": [
    "## Pull test data from the PDS API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09571387-79b1-4d2c-8898-196acdc63e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The peppi method from the PDS API is required to look at the packages\n",
    "import pds.peppi as pep\n",
    "client = pep.PDSRegistryClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0afa6e-40a5-4296-a929-6869718ed826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of seimic data products matching the Apollo PSE seismic data for the day of seismic data that we're interested in.\n",
    "# We can use the datetime format and the method strptime to get the correct day\n",
    "day_to_download = '1970-03-25'\n",
    "day = datetime.strptime(day_to_download, \"%Y-%m-%d\")\n",
    "\n",
    "# Use the the .after and .before methods to only isolate the day that we're interested in\n",
    "products = pep.Products(client).of_collection(\"urn:nasa:pds:apollo_pse:data_seed::1.0\").after(day+dt.timedelta(days=1)).before(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421a383-6e42-4d11-85a3-e6c5648c877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this particular case, we only care about the seismic data, so we will filter based on that parameter\n",
    "seismic_data_products = [p for p in products if p.properties[\"apollo:Seismic_Parameters.apollo:pse_data_type\"][0] == \"waveform\"]\n",
    "\n",
    "# See the number of products available for this time period\n",
    "print(f'{len(seismic_data_products)} Data Product(s) Available!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db17c2-b052-4a0b-a561-f8e97f9dca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each seismic data products has a significant amount of information that we can choose to filter by\n",
    "# Let's take a look at the first product\n",
    "pprint(seismic_data_products[0].properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3042d-6669-48b6-a90a-7b1f20c1fd23",
   "metadata": {},
   "source": [
    "Wow, this is a lot of information! For our uses though, we only really care about the information related to the apollo missions, which starts with the text string \"apollo\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f499041-64ba-420a-a3dc-aed1698e4602",
   "metadata": {},
   "source": [
    "## Let's filter our results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d81aae2-9939-437f-af1f-03c9f86b4172",
   "metadata": {},
   "source": [
    "In specific, we are interested in looking at only seismic data in the **miniseed** standard format, for **Apollo 12** for this day, and only in the **vertical direction broadband** seismic instrument. The information can be found in these labels: \n",
    "\n",
    "`apollo:Observation_Information.apollo:product_type` = **'Mini-SEED'**\n",
    "\n",
    "`apollo:Seismic_Parameters.apollo:station` = **'S12'**\n",
    "\n",
    "`apollo:Seismic_Parameters.apollo:channel` = **'MHZ'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0897115-8f92-497a-804d-e468bce94dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can double check to make sure that the labels correspond to what we're looking for\n",
    "print(seismic_data_products[0].properties['apollo:Observation_Information.apollo:product_type'])\n",
    "print(seismic_data_products[0].properties['apollo:Seismic_Parameters.apollo:station'])\n",
    "print(seismic_data_products[0].properties['apollo:Seismic_Parameters.apollo:channel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2260ab-436b-483c-aca3-f2bc643ddfcb",
   "metadata": {},
   "source": [
    "We can see that the first product that we selected does not correspond to the broadband vertical channel. In fact, this corresponds to the dataless miniseed metadata file. It's an important file that is used to conduct particular processing to the data, but it's not what we need at the moment! Let's filter the data products based on our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349e4a3-660b-4dca-8a8f-276228901fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct filtering\n",
    "type_param = \"Mini-SEED\"\n",
    "sta_param = \"S12\"\n",
    "chan_param = \"MHZ\"\n",
    "seismic_data_products = [p for p in seismic_data_products if p.properties[\"apollo:Observation_Information.apollo:product_type\"][0] == type_param]\n",
    "seismic_data_products = [p for p in seismic_data_products if p.properties[\"apollo:Seismic_Parameters.apollo:station\"][0] == sta_param]\n",
    "seismic_data_products = [p for p in seismic_data_products if p.properties[\"apollo:Seismic_Parameters.apollo:channel\"][0] == chan_param]\n",
    "\n",
    "print(f'{len(seismic_data_products)} Data Product(s) Available! Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79426926-0af8-40f6-b4e4-c0e3a7cc522f",
   "metadata": {},
   "source": [
    "# Plot the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b97034-9a31-48dd-9e93-a4a0c73cb202",
   "metadata": {},
   "source": [
    "Using the Python library Obspy, we can look at the data without having to actually download it by pointing to its url!\n",
    "\n",
    "The url can be found using the `ops:Data_File_Info.ops:file_ref` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b46b06-21dc-4fec-9740-a56e1c68a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_path = seismic_data_products[0].properties['ops:Data_File_Info.ops:file_ref'][0]\n",
    "print(url_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a95918f-6f85-4a2e-a349-861a3d06211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the stream and trace using obspy\n",
    "st = read(url_path)\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42407b6-f2b6-4aee-80f4-8c3338bd1323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at the miniseed header trace and see that it matches the pds label \n",
    "tr = st[0]\n",
    "tr.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f80f8-e85d-4712-8dc1-351acbb9f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's be a bit more neat about reading in the information from the url\n",
    "def read_mseed(infile):\n",
    "    \"\"\"\n",
    "    Reads in the data using the obspy utility and returns the time and velocity measurements\n",
    "    \n",
    "    :param infile: [str] Path to file to read\n",
    "    \"\"\"\n",
    "    # Read the data as a stream and extract the trace\n",
    "    st = read(infile)\n",
    "    tr = st[0]\n",
    "    \n",
    "    return tr, tr.times(), tr.data\n",
    "\n",
    "tr, timedata, valdata_raw = read_mseed(url_path)\n",
    "channame = tr.stats.channel\n",
    "trace_start = tr.stats.starttime\n",
    "station = tr.stats.station\n",
    "day = f'{trace_start.year}-{str(trace_start.month).zfill(2)}-{str(trace_start.day).zfill(2)}' \n",
    "print(f'The channel name is {channame}')\n",
    "print(f'The day used is {day}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3484cb3e-9c53-412a-9cc5-0ede40d29055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw data\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "# Plot the raw data\n",
    "ax0 = plt.subplot(1, 1, 1)\n",
    "ax0.plot(timedata, valdata_raw, color='gray')\n",
    "ax0.set_xlim((timedata[0], timedata[-1]))\n",
    "ax0.set_xlabel('Time (sec)', fontweight='bold')\n",
    "ax0.set_ylabel('Raw Velocity (digital counts)', fontweight='bold')\n",
    "fig.suptitle(f'Raw seismic data for {channame} channel and station {station}  ({day})', fontweight='bold')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e53f9-4a3b-4a6d-8c7d-ff2408f8c29a",
   "metadata": {},
   "source": [
    "# Process the data from machine counts to physical units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5baba-fadf-45c6-8e93-c7c840f37a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some additional libraries\n",
    "from obspy.signal.invsim import cosine_taper\n",
    "from scipy.interpolate import interp1d\n",
    "from obspy import read_inventory\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81060c56-4212-46ff-adfe-8827c60586a9",
   "metadata": {},
   "source": [
    "In order to process the data from machine counts to physical units, we'll need something called a *dataless seed*, which is a type of seed file that only contains metadata. This is a standalone file with the metadata file name:\n",
    "\n",
    "`apollo:Metadata_Location.apollo:metadata_file_name` = **dataless.xa.0.seed**\n",
    "\n",
    "The function that we need to use from osbpy is called `read_inventory`, which works similar to (unlike `obspy.read`) without having to download the file. \n",
    "\n",
    "\n",
    "**!!!TODO!!!**: Find a good way to filter for the dataless URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7885a76-b88a-4919-bb4a-01a038aa124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Query the data again\n",
    "# day_to_download = '1970-03-25'\n",
    "# day = datetime.strptime(day_to_download, \"%Y-%m-%d\")\n",
    "# products = pep.Products(client).of_collection(\"urn:nasa:pds:apollo_pse:data_seed::1.0\").after(day+dt.timedelta(days=1)).before(day)\n",
    "\n",
    "# # Get the data products and filter\n",
    "# seismic_data_products = [p for p in products if p.properties[\"apollo:Seismic_Parameters.apollo:pse_data_type\"][0] == \"waveform\"]\n",
    "# seismic_data_products = [p for p in seismic_data_products if p.properties[\"apollo:Metadata_Location.apollo:metadata_file_name\"][0] == 'dataless.xa.0.seed']\n",
    "\n",
    "# # Let's check we got the right one\n",
    "# seismic_data_products[0].properties['apollo:Metadata_Location.apollo:metadata_file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19875701-21a4-4221-8c17-03ab39a4bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This is incorrect\n",
    "# url_dataless = seismic_data_products[0].properties['ops:Data_File_Info.ops:file_ref'][0]\n",
    "url_dataless = 'https://pds-geosciences.wustl.edu/lunar/urn-nasa-pds-apollo_pse/data/xa/metadata/dataless.xa.0.seed'\n",
    "print(url_dataless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6d07f-15ad-4b7a-8b26-f473c73abc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some subroutines required for the data processing\n",
    "def running_median(seq, win):\n",
    "    \"\"\"\n",
    "    Conducts a running median on the data\n",
    "\n",
    "    :param seq: [Vector] Input data\n",
    "    :param win: [Integer] Size of the window (in samples)\n",
    "    \"\"\"\n",
    "\n",
    "    samples = np.arange(len(seq))\n",
    "    medians = []\n",
    "\n",
    "    window_middle = int(np.ceil(win / 2))\n",
    "\n",
    "    for ind in np.arange(len(seq)):\n",
    "\n",
    "        if ind <= window_middle:\n",
    "            medians.append(np.median(abs(seq[0:win])))\n",
    "\n",
    "        if ind >= len(seq) - window_middle:\n",
    "            medians.append(np.median(abs(seq[len(seq) - win:len(seq)])))\n",
    "\n",
    "        if window_middle < ind < len(seq) - window_middle:\n",
    "            medians.append(np.median(abs(seq[ind - int(np.floor(win / 2)):ind + int(np.floor(win / 2))])))\n",
    "\n",
    "    return np.array(medians)\n",
    "\n",
    "\n",
    "def despike(input_t, input_d, fs):\n",
    "    \"\"\"\n",
    "    Despikes the data according to Budlow 2005\n",
    "\n",
    "    :param input_t: [Vector] Interpolated time\n",
    "    :param input_d: [Vector] Interpolated data\n",
    "    :param fs: [Float] Sampling frequency\n",
    "    :param instrument_type: [String] Type of instrument [lp = long period, sp = short period]\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute a running median on the data\n",
    "    # The window size should be 2 minutes (120 seconds) and odd\n",
    "    window_size = int(fs * 120)\n",
    "    if window_size % 2 == 0:\n",
    "        window_size = window_size + 1\n",
    "    med = running_median(input_d, window_size)\n",
    "\n",
    "    # Find values greater than 5 times the running median\n",
    "    med_multiplier = 5.\n",
    "    indices_to_remove = []\n",
    "    for ind in np.arange(len(input_d)):\n",
    "        if input_d[ind] > abs(med[ind] * med_multiplier) or input_d[ind] < -1 * abs(med[ind] * med_multiplier):\n",
    "            indices_to_remove.append(ind)\n",
    "\n",
    "    # Remove those values from the time and data\n",
    "    input_t_del = np.delete(input_t, indices_to_remove)\n",
    "    input_d_del = np.delete(input_d, indices_to_remove)\n",
    "\n",
    "    # If we remove the last value in the dat, we run into trouble because it can't finish interpolation\n",
    "    # If it's missing, append a zero value to the data at the end. We have a total of four cases.\n",
    "    # Missing beginning\n",
    "    if not input_t_del[0] == input_t[0] and input_t_del[-1] == input_t[-1]:\n",
    "        input_t_del_fin = np.insert(input_t_del, 0, input_t[0])\n",
    "        input_d_del_fin = np.insert(input_d_del, 0, 0)\n",
    "\n",
    "    # Missing end\n",
    "    if input_t_del[0] == input_t[0] and not input_t_del[-1] == input_t[-1]:\n",
    "        input_t_del_fin = np.append(input_t_del, input_t[-1])\n",
    "        input_d_del_fin = np.append(input_d_del, 0)\n",
    "\n",
    "    # Both missing\n",
    "    if not input_t_del[0] == input_t[0] and not input_t_del[-1] == input_t[-1]:\n",
    "        input_t_del_fixbeg = np.insert(input_t_del, 0, input_t[0])\n",
    "        input_d_del_fixbeg = np.insert(input_d_del, 0, 0)\n",
    "        input_t_del_fin = np.append(input_t_del_fixbeg, input_t[-1])\n",
    "        input_d_del_fin = np.append(input_d_del_fixbeg, 0)\n",
    "\n",
    "    # Nothing missing\n",
    "    if input_t_del[0] == input_t[0] and input_t_del[-1] == input_t[-1]:\n",
    "        input_t_del_fin = input_t_del\n",
    "        input_d_del_fin = input_d_del\n",
    "\n",
    "    # Interpolate over the missing values\n",
    "    # We can call on our original input_t variable\n",
    "    f2 = interp1d(input_t_del_fin, input_d_del_fin)\n",
    "    d_interp2 = f2(input_t)\n",
    "\n",
    "    return d_interp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a6df2-1fdc-41ec-9205-1f90a2f8854c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_data(indata, dataless_seed):\n",
    "    \"\"\"\n",
    "    Processes the data: removes mean, tapers, and removes the instrument response (converting it to physical units)\n",
    "    \n",
    "    :param infile: [str] Path to file to read\n",
    "    :param dataless_seed: [str] Path to the dataless seed, which is required for removing the instrument response\n",
    "    \"\"\"\n",
    "    # Read the data as a stream and extract the trace\n",
    "    st = read(indata)\n",
    "    tr = st[0]\n",
    "    \n",
    "    # Remove the mean and run a cosine taper\n",
    "    tr.data = tr.data - np.mean(tr.data)\n",
    "    taper_function = cosine_taper(len(tr.data), p=0.02)\n",
    "    tr.data = tr.data * taper_function\n",
    "\n",
    "    # Set a bandpass filter and remove the instrument response\n",
    "    # Note: This narrow filter is required for instrument response removal. Otherwise the low frequencies dominate the spectrum. \n",
    "    pre_filt = [0.1, 0.3, 0.9, 1.1]\n",
    "    inv = read_inventory(dataless_seed)\n",
    "    tr.remove_response(inventory=inv, pre_filt=pre_filt, output=\"VEL\",\n",
    "                       water_level=None)\n",
    "\n",
    "    # Interpolate and despike the data\n",
    "    sr = 6.625\n",
    "    delta_target = 1 / sr\n",
    "    tm = tr.times()\n",
    "    times_seconds_interp = np.arange(tm[0], tm[-1] - delta_target, delta_target)\n",
    "    tm_utc = tr.times(type='utcdatetime')\n",
    "    tm_num_interp = [tm_utc[0] + dt.timedelta(seconds=x) for x in\n",
    "                        times_seconds_interp]\n",
    "\n",
    "    f = interp1d(tm, tr.data)\n",
    "    d_interp = f(times_seconds_interp)\n",
    "\n",
    "    d_new = despike(times_seconds_interp, d_interp, 1 / delta_target)\n",
    "\n",
    "    print(f'Processed file {os.path.basename(indata)}')\n",
    "\n",
    "    return times_seconds_interp, d_new\n",
    "\n",
    "proctime, procdata = process_data(url_path, url_dataless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f96e9-c29c-4799-a950-9b3f4614a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "# Plot the raw data\n",
    "ax0 = plt.subplot(2, 1, 1)\n",
    "ax0.plot(timedata, valdata_raw, color='gray')\n",
    "ax0.set_xlim((timedata[0], timedata[-1]))\n",
    "ax0.set_xlabel('Time (sec)', fontweight='bold')\n",
    "ax0.set_ylabel('Raw Velocity (digital counts)', fontweight='bold')\n",
    "\n",
    "# Plot the processed data\n",
    "ax1 = plt.subplot(2, 1, 2)\n",
    "ax1.plot(proctime, procdata)\n",
    "ax1.set_xlim((proctime[0], proctime[-1]))\n",
    "ax1.set_xlabel('Time (sec)', fontweight='bold')\n",
    "ax1.set_ylabel('Processed Velocity (m/s)', fontweight='bold')\n",
    "fig.suptitle(f'Processed seismic data for {channame} channel and station {station}  ({day})', fontweight='bold')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366888d0-0872-409e-921e-9f0491261c41",
   "metadata": {},
   "source": [
    "Compute Spectrograms for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5540bec-0b07-4f01-80a7-abb2a71c9f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional packages\n",
    "from scipy import signal\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081bd1c-136c-4e86-9585-3ef9e6af322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to compute the seismic arrival time of an impact moonquake that occurs in this hour\n",
    "# We use the term evid (event ID) to describe the arrival\n",
    "# The format we will use for datetime is %Y-%m-%dT%H:%M:%S\n",
    "arrival_absolute_str = '1970-03-25T3:32:00'\n",
    "arrival_absolute_dt = dt.datetime.strptime(arrival_absolute_str, '%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "# Convert it to the time in seconds after the start of the hour\n",
    "arrival_rel = (arrival_absolute_dt - trace_start.datetime).seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15fe7f3-0566-460b-b3c1-92099d3fd1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spec(trtime, trvals):\n",
    "    \"\"\"\n",
    "    Computes the spectrograms of the processed data\n",
    "\n",
    "    :param trtime: [vector] Processed time values\n",
    "    :param trvals: [vector] Processed velocity values\n",
    "    \"\"\"\n",
    "    # Get the sample-rate of the processed data. If we don't have an explicit value saved, we can just use differences in the time vector\n",
    "    delta_samples = trtime[1]-trtime[0]\n",
    "    sampling_rate = 1/delta_samples\n",
    "    \n",
    "    # Compute spectrogram\n",
    "    spec_f, spec_t, spec_sxx = signal.spectrogram(trvals, sampling_rate)\n",
    "    \n",
    "    return spec_f, spec_t, spec_sxx\n",
    "\n",
    "# Cycle through each file \n",
    "spec_f, spec_t, spec_sxx = compute_spec(proctime, procdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f757355c-b8a3-4eee-8f6c-8e3d1c2ea660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot the raw data\n",
    "ax0 = plt.subplot(3, 1, 1)\n",
    "ax0.plot(timedata, valdata_raw, color='gray')\n",
    "ax0.set_xlim((timedata[0], timedata[-1]))\n",
    "ax0.set_xlabel('Time (sec)', fontweight='bold')\n",
    "ax0.set_ylabel('Raw Velocity (digital counts)', fontweight='bold')\n",
    "\n",
    "# Plot the processed data\n",
    "ax1 = plt.subplot(3, 1, 2)\n",
    "ax1.plot(proctime, procdata)\n",
    "ax1.set_xlim((proctime[0], proctime[-1]))\n",
    "ax1.set_xlabel('Time (sec)', fontweight='bold')\n",
    "ax1.set_ylabel('Processed Velocity (m/s)', fontweight='bold')\n",
    "\n",
    "# Plot the spectrogram\n",
    "ax2 = plt.subplot(3, 1, 3)\n",
    "ax2.pcolormesh(spec_t, spec_f, spec_sxx, cmap=cm.jet, vmax=5e-18)\n",
    "ax2.set_xlabel('Time (sec)', fontweight='bold')\n",
    "ax2.set_ylabel('Frequency (Hz)', fontweight='bold')\n",
    "\n",
    "# Plot the arrivals\n",
    "ax0.axvline(arrival_rel, c='red')\n",
    "ax1.axvline(arrival_rel, c='red')\n",
    "ax2.axvline(arrival_rel, c='red')\n",
    "    \n",
    "fig.suptitle(f'Data and spectrogram for {channame} channel and station {station}  ({day})', fontweight='bold')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a37df56-9176-480a-b0e8-940a3bb38455",
   "metadata": {},
   "source": [
    "## Cut and export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571c892b-3f2e-4d53-8c65-e4dfb1b97bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import more packages\n",
    "from obspy import UTCDateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bb197-1b5d-4124-b708-48563dfac5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a trim length before and after the arrival (in seconds)\n",
    "pre_arrival_time = 300\n",
    "post_arrival_time = 5200\n",
    "\n",
    "# Use the timedelta function to get the absolute datetime of when to cut the trace\n",
    "pre_arrival_dt = arrival_absolute_dt - dt.timedelta(seconds=pre_arrival_time)\n",
    "post_arrival_dt = arrival_absolute_dt + dt.timedelta(seconds=post_arrival_time)\n",
    "\n",
    "# Obspy uses utctime, so use the UTCDateTime function to convert it to that format\n",
    "pre_arrival_utc = UTCDateTime(pre_arrival_dt)\n",
    "post_arrival_utc = UTCDateTime(post_arrival_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c0583-87b9-416c-9e56-775478925199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an output directory\n",
    "outdir = f'{os.getcwd()}test_output/'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "# Cut the file\n",
    "st = read(url_path)\n",
    "st.trim(pre_arrival_utc, post_arrival_utc)\n",
    "st.write(f'{outdir}{os.path.basename(url_path)}_cut.mseed', format='MSEED')\n",
    "print(f'Cut file {os.path.basename(url_path)}_cut.mseed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b46c2f-baa7-4520-a676-84eab1fdbf08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed11ecd-7999-4720-b5ca-e1f970cbf0de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
